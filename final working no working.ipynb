{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209cbbf0-e9c3-4249-97cd-46f768f4b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the YOLO model and action recognition model\n",
    "yolo_model = YOLO(r'C:\\Users\\poory\\Desktop\\work\\bonji\\action detection\\track\\best.pt')\n",
    "action_recognition_model = load_model('action_recognition_model.h5')\n",
    "\n",
    "# Video and frame handling parameters\n",
    "video_path = r'C:\\Users\\poory\\Desktop\\work\\bonji\\action detection\\track\\video\\croped2.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret = True\n",
    "frame_skip_interval = 20\n",
    "frame_count = 0\n",
    "IMG_HEIGHT, IMG_WIDTH = 64, 64\n",
    "SEQ_LENGTH = 20\n",
    "\n",
    "# List of actions (assuming the same order as used during training)\n",
    "ACTIONS_LIST = ['sleeping', 'using the phone', 'sitting and working', 'sitting and talking', 'working', 'eating food']\n",
    "\n",
    "# Initialize a deque to store the frames for each person\n",
    "person_frames = {}\n",
    "\n",
    "while ret:\n",
    "    ret, frame = cap.read()\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames for processing efficiency\n",
    "    if frame_count % frame_skip_interval != 0:\n",
    "        continue\n",
    "\n",
    "    # Track people in the frame\n",
    "    results = yolo_model.track(frame, persist=True)\n",
    "    tracked_frame = results[0].plot()\n",
    "\n",
    "    # Loop through each detected person in the frame\n",
    "    for detection in results[0].boxes:\n",
    "        # Check if the detection label is 'person'\n",
    "        if detection.cls == 0:  # assuming '0' corresponds to 'person' in your model\n",
    "            x1, y1, x2, y2 = map(int, detection.xyxy[0])  # Get bounding box coordinates\n",
    "            person_id = int(detection.id)  # Get unique ID for the tracked person\n",
    "            \n",
    "            # Crop and preprocess the bounding box image\n",
    "            person_frame = frame[y1:y2, x1:x2]\n",
    "            resized_person_frame = cv2.resize(person_frame, (IMG_HEIGHT, IMG_WIDTH))\n",
    "            normalized_person_frame = resized_person_frame / 255.0\n",
    "\n",
    "            # Initialize frame deque for new persons\n",
    "            if person_id not in person_frames:\n",
    "                person_frames[person_id] = deque(maxlen=SEQ_LENGTH)\n",
    "\n",
    "            # Add the processed frame to the person's deque\n",
    "            person_frames[person_id].append(normalized_person_frame)\n",
    "\n",
    "            # If we have enough frames for action recognition, predict the action\n",
    "            if len(person_frames[person_id]) == SEQ_LENGTH:\n",
    "                # Prepare the frame sequence as input to the action recognition model\n",
    "                frame_sequence = np.expand_dims(person_frames[person_id], axis=0)\n",
    "                \n",
    "                # Predict the action\n",
    "                predicted_probs = action_recognition_model.predict(frame_sequence)[0]\n",
    "                predicted_label = np.argmax(predicted_probs)\n",
    "                predicted_action = ACTIONS_LIST[predicted_label]\n",
    "\n",
    "                # Map action to \"working\" or \"not working\"\n",
    "                if predicted_action in ['sitting and working', 'working']:\n",
    "                    action_status = \"working\"\n",
    "                else:\n",
    "                    action_status = \"not working\"\n",
    "\n",
    "                # Display the working status on the bounding box\n",
    "                cv2.putText(tracked_frame, action_status, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with action recognition results\n",
    "    cv2.imshow('Person Tracking and Action Recognition', tracked_frame)\n",
    "\n",
    "    # Break on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
